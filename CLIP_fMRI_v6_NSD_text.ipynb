{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c114eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP model for fMRI text mapping\n",
    "## using simple Ridge regression\n",
    "# worked on algonauts dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83ec151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import timm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lightly.loss import BarlowTwinsLoss\n",
    "from lightly.loss import NTXentLoss\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ebfe4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae0e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac4e82",
   "metadata": {},
   "source": [
    "### Things dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766431e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load image features\n",
    "# print('IMAGES -------')\n",
    "# loaded = torch.load('../resources/clip_img_feat_224')\n",
    "# train_feat_img = loaded['train_feat']\n",
    "# test_feat_img = loaded['test_feat']\n",
    "# train_feat_img = train_feat_img.squeeze(1)\n",
    "# test_feat_img = test_feat_img.squeeze(1)\n",
    "# train_feat_img = train_feat_img.detach().cpu().numpy()\n",
    "# test_feat_img = test_feat_img.detach().cpu().numpy()\n",
    "# print(train_feat_img.shape, test_feat_img.shape)\n",
    "# print(np.max(train_feat_img), np.min(train_feat_img))\n",
    "# scaler1 = MinMaxScaler()\n",
    "# train_feat_img = scaler1.fit_transform(train_feat_img)\n",
    "# test_feat_img = scaler1.transform(test_feat_img)\n",
    "# print(np.max(train_feat_img), np.min(train_feat_img))\n",
    "# train_feat_img = train_feat_img.astype('float32')\n",
    "# test_feat_img = test_feat_img.astype('float32')\n",
    "\n",
    "# # # load text features\n",
    "# # print('TEXT -------')\n",
    "# # loaded = torch.load('../resources/clip_txt_feat_224')\n",
    "# # train_feat_img = loaded['train_feat']\n",
    "# # test_feat_img = loaded['test_feat']\n",
    "# # train_feat_img = train_feat_img.squeeze(1)\n",
    "# # test_feat_img = test_feat_img.squeeze(1)\n",
    "# # train_feat_img = train_feat_img.detach().cpu().numpy()\n",
    "# # test_feat_img = test_feat_img.detach().cpu().numpy()\n",
    "# # print(train_feat_img.shape, test_feat_img.shape)\n",
    "# # print(np.max(train_feat_img), np.min(train_feat_img))\n",
    "# # scaler1 = MinMaxScaler()\n",
    "# # train_feat_img = scaler1.fit_transform(train_feat_img)\n",
    "# # test_feat_img = scaler1.transform(test_feat_img)\n",
    "# # print(np.max(train_feat_img), np.min(train_feat_img))\n",
    "# # train_feat_img = train_feat_img.astype('float32')\n",
    "# # test_feat_img = test_feat_img.astype('float32')\n",
    "\n",
    "\n",
    "# # FMRI\n",
    "# print('FMRI ----------')\n",
    "# train_file = np.load('../resources/sub2-fmri-train.npy')\n",
    "# test_file = np.load('../resources/sub2-fmri-test.npy')\n",
    "# train_file = train_file.T\n",
    "# test_file = test_file.T\n",
    "# print(train_file.shape, test_file.shape)\n",
    "# print(np.max(train_file), np.min(train_file))\n",
    "# scaler2 = MinMaxScaler()\n",
    "# train_file = scaler2.fit_transform(train_file)\n",
    "# test_file = scaler2.transform(test_file)\n",
    "# print(np.max(train_file), np.min(train_file))\n",
    "# train_feat_fmri = train_file.astype('float32')\n",
    "# test_feat_fmri = test_file.astype('float32')\n",
    "\n",
    "# # # FMRI embeddings\n",
    "# # train_file = np.load('../resources/ae_train_fmri_embed.npy')\n",
    "# # test_file = np.load('../resources/ae_test_fmri_embed.npy')\n",
    "# # print(train_file.shape, test_file.shape)\n",
    "# # print(np.max(train_file), np.min(train_file))\n",
    "# # train_file = train_file.astype('float32')\n",
    "# # test_file = test_file.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21612113-918a-48a7-96d1-c153b22f0263",
   "metadata": {},
   "source": [
    "### Algonauts NSD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/Documents/research/projects/algonauts_2023/resources/algonauts_2023_challenge_data/'\n",
    "data_dir = os.path.join(data_dir, 'subj01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfee4929-d919-423e-9986-c39a8888d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load image features\n",
    "# loaded = torch.load(data_dir+'/clip_nsd_img_feat_224')\n",
    "# feat_img = loaded['train_feat']\n",
    "# feat_img = feat_img.squeeze(1)\n",
    "# feat_img = feat_img.detach().cpu().numpy()\n",
    "# print(feat_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ecc59dd-b490-45af-94be-8087cc96a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train test split because given test set does not have fMRI\n",
    "# train_feat_img = feat_img[:7000]\n",
    "# test_feat_img = feat_img[7000:]\n",
    "# print(train_feat_img.shape, test_feat_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b1ba6c-1027-40eb-a1fd-1fee0df17c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize\n",
    "# print(np.max(train_feat_img), np.min(test_feat_img))\n",
    "# scaler1 = MinMaxScaler()\n",
    "# train_feat_img = scaler1.fit_transform(train_feat_img)\n",
    "# test_feat_img = scaler1.transform(test_feat_img)\n",
    "# print(np.max(train_feat_img), np.min(train_feat_img))\n",
    "# train_feat_img = train_feat_img.astype('float32')\n",
    "# test_feat_img = test_feat_img.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b85f72a-947e-4ac2-ba27-8f381c71ebe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49205, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsd_captions = pd.read_csv(\"/home/Documents/research/projects/algonauts_2023/resources/nsd_captions.csv\")\n",
    "nsd_captions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f2c90c3-6a87-4f20-8335-3136854cc654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>nsd_id</th>\n",
       "      <th>coco_id</th>\n",
       "      <th>coco_caption_id</th>\n",
       "      <th>coco_caption</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train-0001_nsd-00013.png</td>\n",
       "      <td>13</td>\n",
       "      <td>24610</td>\n",
       "      <td>162385</td>\n",
       "      <td>A disorderly living area is free from decorati...</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train-0001_nsd-00013.png</td>\n",
       "      <td>13</td>\n",
       "      <td>24610</td>\n",
       "      <td>168364</td>\n",
       "      <td>A small living space with a couch, desk, chair...</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train-0001_nsd-00013.png</td>\n",
       "      <td>13</td>\n",
       "      <td>24610</td>\n",
       "      <td>170512</td>\n",
       "      <td>There is a laptop computer in a room with a co...</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train-0001_nsd-00013.png</td>\n",
       "      <td>13</td>\n",
       "      <td>24610</td>\n",
       "      <td>175837</td>\n",
       "      <td>A simple room with minimal decor and furniture.</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-0001_nsd-00013.png</td>\n",
       "      <td>13</td>\n",
       "      <td>24610</td>\n",
       "      <td>178792</td>\n",
       "      <td>A small room with a computer and bookcase.</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   image_id  nsd_id  coco_id  coco_caption_id  \\\n",
       "0  train-0001_nsd-00013.png      13    24610           162385   \n",
       "1  train-0001_nsd-00013.png      13    24610           168364   \n",
       "2  train-0001_nsd-00013.png      13    24610           170512   \n",
       "3  train-0001_nsd-00013.png      13    24610           175837   \n",
       "4  train-0001_nsd-00013.png      13    24610           178792   \n",
       "\n",
       "                                        coco_caption split  \n",
       "0  A disorderly living area is free from decorati...  Test  \n",
       "1  A small living space with a couch, desk, chair...  Test  \n",
       "2  There is a laptop computer in a room with a co...  Test  \n",
       "3   A simple room with minimal decor and furniture.   Test  \n",
       "4        A small room with a computer and bookcase.   Test  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsd_captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45435fb6-bc59-4014-be6d-9675612dd417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9841, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_nth_rows(dataframe, n):\n",
    "    # Extract every nth row\n",
    "    return dataframe.iloc[n-1::n]\n",
    "\n",
    "nsd_captions_subset = extract_nth_rows(nsd_captions, 5)\n",
    "nsd_captions_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36cd7868-bd60-4ae3-a642-633ba595bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_list = nsd_captions_subset.coco_caption.tolist()\n",
    "train_captions = captions_list[:7000]\n",
    "test_captions = captions_list[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ff52ac-b6be-47bd-8bcb-8a7a353a2cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9841, 512)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load text feat\n",
    "text_feat = np.load(data_dir+'/clip_nsd_caption_feat.npy')\n",
    "text_feat = text_feat.squeeze(1)\n",
    "text_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd9b5f4-bd68-4a1d-b046-f67390d82e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 512) (2841, 512)\n"
     ]
    }
   ],
   "source": [
    "# train test split because given test set does not have fMRI\n",
    "train_feat_txt = text_feat[:7000]\n",
    "test_feat_txt = text_feat[7000:]\n",
    "print(train_feat_txt.shape, test_feat_txt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f55e2c90-bce6-478f-8e91-4f5d25728a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.483438 -3.940516\n",
      "1.0000001 -0.1722239\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "print(np.max(train_feat_txt), np.min(test_feat_txt))\n",
    "scaler2 = MinMaxScaler()\n",
    "train_feat_txt = scaler2.fit_transform(train_feat_txt)\n",
    "test_feat_txt = scaler2.transform(test_feat_txt)\n",
    "print(np.max(train_feat_txt), np.min(test_feat_txt))\n",
    "train_feat_txt = train_feat_txt.astype('float32')\n",
    "test_feat_txt = test_feat_txt.astype('float32')\n",
    "# train_feat_txt, test_feat_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1c03986-aefa-4886-9a2c-3b436d19c265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n"
     ]
    }
   ],
   "source": [
    "# load fMRI training data\n",
    "fmri_dir = os.path.join(data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "085f666c-5fc7-405c-b463-ed9e6ef30096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 19004) (2841, 19004)\n"
     ]
    }
   ],
   "source": [
    "train_feat_fmri_lh = lh_fmri[:7000]\n",
    "test_feat_fmri_lh = lh_fmri[7000:]\n",
    "train_feat_fmri_rh = rh_fmri[:7000]\n",
    "test_feat_fmri_rh = rh_fmri[7000:]\n",
    "print(train_feat_fmri_lh.shape, test_feat_fmri_lh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bfbbe96-812a-4022-997f-ad526a9191e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2208066 -5.5488534\n",
      "1.0000001 0.0\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "print(np.max(train_feat_fmri_lh), np.min(train_feat_fmri_lh))\n",
    "scaler3 = MinMaxScaler()\n",
    "train_feat_fmri_lh = scaler3.fit_transform(train_feat_fmri_lh)\n",
    "test_feat_fmri_lh = scaler3.transform(test_feat_fmri_lh)\n",
    "print(np.max(train_feat_fmri_lh), np.min(train_feat_fmri_lh))\n",
    "train_feat_fmri_lh = train_feat_fmri_lh.astype('float32')\n",
    "test_feat_fmri_lh = test_feat_fmri_lh.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cfa370f-a6cf-49d3-bcc6-e1d09ebb205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2803955 -6.224722\n",
      "1.0000001 0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(train_feat_fmri_rh), np.min(train_feat_fmri_rh))\n",
    "scaler4 = MinMaxScaler()\n",
    "train_feat_fmri_rh = scaler4.fit_transform(train_feat_fmri_rh)\n",
    "test_feat_fmri_rh = scaler4.transform(test_feat_fmri_rh)\n",
    "print(np.max(train_feat_fmri_rh), np.min(train_feat_fmri_rh))\n",
    "train_feat_fmri_rh = train_feat_fmri_rh.astype('float32')\n",
    "test_feat_fmri_rh = test_feat_fmri_rh.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc54b8c-c724-44d1-9aeb-0f6069f6088e",
   "metadata": {},
   "source": [
    "#### Model on text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15f3b5fb-d397-4426-aa68-8dd079f12a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feat_txt, test_feat_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9406f8a-9f08-4868-bb84-e30a1dc70e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 512) (2841, 512)\n",
      "CPU times: user 26.4 s, sys: 2.26 s, total: 28.7 s\n",
      "Wall time: 8.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf_lh = Ridge(alpha=100)\n",
    "# clf = ExtraTreesRegressor(n_estimators=500)\n",
    "clf_lh.fit(train_feat_fmri_lh, train_feat_txt)\n",
    "train_feat_img_pred_lh = clf_lh.predict(train_feat_fmri_lh)\n",
    "test_feat_img_pred_lh = clf_lh.predict(test_feat_fmri_lh)\n",
    "print(train_feat_img_pred_lh.shape, test_feat_img_pred_lh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66709aa9-4823-44a4-b33b-5e459635fc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 512) (2841, 512)\n",
      "CPU times: user 28.2 s, sys: 2.33 s, total: 30.5 s\n",
      "Wall time: 9.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf_rh = Ridge(alpha=100)\n",
    "# clf = ExtraTreesRegressor(n_estimators=500)\n",
    "clf_rh.fit(train_feat_fmri_rh, train_feat_txt)\n",
    "train_feat_img_pred_rh = clf_rh.predict(train_feat_fmri_rh)\n",
    "test_feat_img_pred_rh = clf_rh.predict(test_feat_fmri_rh)\n",
    "print(train_feat_img_pred_rh.shape, test_feat_img_pred_rh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8c271ab-560f-4325-a477-0da962bff941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 512) (2841, 512)\n"
     ]
    }
   ],
   "source": [
    "train_feat_img_pred = np.mean([train_feat_img_pred_lh, train_feat_img_pred_rh],axis=0)\n",
    "test_feat_img_pred = np.mean([test_feat_img_pred_lh, test_feat_img_pred_rh],axis=0)\n",
    "print(train_feat_img_pred.shape, test_feat_img_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5b2faf0-e8d9-4637-9cc3-b31cc53b820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize predictions\n",
    "# test_feat_img_pred = scaler2.transform(test_feat_img_pred)      # didnt work\n",
    "# print(np.max(test_feat_img_pred), np.min(test_feat_img_pred))\n",
    "# test_feat_img_pred = test_feat_img_pred.astype('float32')\n",
    "# print(np.max(test_feat_img_pred), np.min(test_feat_img_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b7974c9-ce18-456b-8220-02262f4cc6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016338972\n",
      "0.0082784835\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(test_feat_txt, test_feat_img_pred))\n",
    "print(mean_squared_error(train_feat_txt, train_feat_img_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "597255f8-888c-499a-bfda-30857cac4207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20544,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rh_weights = np.mean(clf_rh.coef_, axis=0)\n",
    "clf_rh_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d20a0e27-0d22-4f0a-a370-8fc597fc2eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19004,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lh_weights = np.mean(clf_lh.coef_, axis=0)\n",
    "clf_lh_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193dacc-2331-49e3-938a-0a47c9d8a729",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "211ebbf4-38b1-4ee2-b90a-20d761fad06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aa41fd8-7e63-4642-b005-08fc08e523f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_img_embed = test_feat_img_pred[test_num]\n",
    "sample_test_img_embed = sample_test_img_embed.reshape(1, -1)\n",
    "sample_test_img_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd6208f7-053d-4b0b-94aa-a7b59246c3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = cosine_similarity(sample_test_img_embed, train_feat_txt)\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6781b05-71c5-4e0f-96d8-bf0a479cacdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted labels:\n",
      "4306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4306"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_inx = 0\n",
    "indices = np.argsort(similarities.flatten())[-1:]\n",
    "print(\"predicted labels:\")\n",
    "for idx in indices:\n",
    "    print(idx)\n",
    "    pred_inx = idx\n",
    "pred_inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eff83dbb-b09b-4dd8-97cc-3d4155c209f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual -  A man that is sitting down holding bread.\n",
      "predicted -  A man is sitting with another and holding a plate of food.\n"
     ]
    }
   ],
   "source": [
    "print('Actual - ', test_captions[test_num])\n",
    "print('predicted - ', train_captions[pred_inx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
